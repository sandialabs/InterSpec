<?xml version="1.0" encoding="UTF-8"?>

<!-- Warning: LLM capabilities are totally untested, and currently so incomplete that they are useless
for anything other than having a little fun with.  Also, there has been zero safety/security checks
or considerations, and it has only been tested to kinda-sorta work with a few LLMs.
-->

<!-- 
This is an example configuration file to enable using the "LLM Assistant" tool in InterSpec, and/or
for enabling the MCP server to allow an external LLM program to control analysis, or query for information.

To enable LLM capabilities, prefferably copy this file to your OS provided data directory for InterSpec (e.g., on Windows it is 
usually "C:\Users\<user>\AppData\Roaming\InterSpec", 
and macOS "/Users/<user>/Library/Containers/gov.sandia.macOS.InterSpec/Data/Library/Application Support/sandia.InterSpec",
but see "Help" > "About InterSpec" > "Data" for the actual location), then edit it to enable the LLM API, putting in your 
endpoint URL and bearer token, and/or enable the MCP server and add the required bearer token.

For desktop versions of InterSpec, if this configuration file is not valid (invalid XML, missing any field, not expected field 
data type, etc), the LLM capabilities will not be enabled, and only a message will be printed to stderr.

Changes to this file will not take effect until you quit all currently open instances of InterSpec, and then start it again.
-->
<LlmConfig version="0">

  <!-- LLM API Settings - this is for the interactive tool inside InterSpec that uses a LLM -->
  <LlmApi version="0">
    <Enabled>false</Enabled>

    <!-- A number of providers work, including locally running LLM models -->
    <ApiEndpoint>https://api.openai.com/v1/chat/completions</ApiEndpoint>
    <BearerToken>sk-...</BearerToken>

    <!-- Model specification with optional reasoning attribute
      The reasoning attribute can be:
      - Boolean (true/false): For OpenRouter-style reasoning (adds reasoning: {enabled: true} to requests)
        Example: <Model reasoning="true">anthropic/claude-sonnet-4.5</Model>

      - Effort level (low/medium/high): For OpenAI o1/o3-style reasoning (adds reasoning_effort to requests)
        Example: <Model reasoning="medium">openai/o3-mini</Model>
        - low: Faster, less thorough reasoning
        - medium: Balanced reasoning (recommended)
        - high: Most thorough reasoning, slower

      If reasoning attribute is not specified, reasoning is disabled.
    -->
    <Model>gpt-5-mini</Model>

    <MaxTokens>62565</MaxTokens>
    <ContextLengthLimit>131072</ContextLengthLimit>

    <!-- Optional model temperature
      0.0 (Deterministic): The model will be very focused and usually give the same answer every time.
      0.2 - 0.5 (Balanced): Recommended; provides "creativity", while remaining grounded in your spectroscopy tools.
      0.7 (Creative): More creative, but increases risk of the model hallucinating tool arguments that don't exist.
    -->
    <ModelTemperature>0.2</ModelTemperature>

    <!--  A HTTP enpoint to post questions to, to do deeper research.
     At the moment, this is a seperate service that is not distributed, so it likely will not be of use to anyone besides the primary InterSpec developer.

     An example value you would fill in for this field is: "http://localhost:8000/query".
     If this field is non-empty, then the `deep_research` tool-call will be enabled, which will cause
     this tool call to make a shell command to have `curl` make a call like:
     ```
       curl -X 'POST' 'http://localhost:8000/query' -H 'accept: application/json' -H 'Content-Type: application/json' -d '{"messages": [{"content": "what is the primary use of Ba133?"}], "corpora": ["my_gamma_spec_dataset"]}'
     ```

     If you leave this field blank, the `deep_research` tool-call will not be available.
     It is very recomended you do not put a value in this field.
    -->
    <DeepResearchUrl></DeepResearchUrl>

    <!-- Optional debug logging configuration. If empty or not present, no debug logging will be done.
         Valid values:
         - "stdout" or "stderr": Log debug messages to standard output or error streams
         - A file path: Log debug messages to the specified file (will be created/overwritten)

         Debug logging includes detailed information about LLM interactions, state transitions,
         tool calls, and internal processing steps useful for development and troubleshooting.
    -->
    <!-- <DebugFile>llm_debug.log</DebugFile> -->
    <!-- <DebugFile>stdout</DebugFile> -->
  </LlmApi>
  
  <!-- MCP Server Settings - this is for InterSpec to create a MCP server you can use with other applications.
  
  It will be avaiable at http://127.0.0.1:<port>/mcp-api, where port is the same on InterSpec instance is being
  served on - see `HttpPortToServeOn` in InterSpec_app_settings.json - by default it is a random high-numbered
  port.
   -->
  <McpServer version="0">
    <Enabled>false</Enabled>

    <!-- The bearer token required to access the MCP instance. If empty, no authorization will be required.
    But you must change this from "INVALID-BEARER-TOKEN" if you enable this capability.
     -->
    <BearerToken>INVALID-BEARER-TOKEN</BearerToken>
  </McpServer>
</LlmConfig>
